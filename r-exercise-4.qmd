---
title: "Regression in R"
description: "Basic Concepts Regression Techniques"
author: "Bharath Velamala"
format: 
    html:
      theme: yeti
editor: visual
toc: true
code-overflow: wrap
code-annotations: hover
execute: 
  warning: false
---

## Install packages

Installing the packages used.

```{r r_packages, message = FALSE, output=FALSE}
# Required packages
if (!require(pacman))
  install.packages("pacman")

pacman::p_load(tidymodels,
               tidyverse,
               ranger,
               randomForest,
               glmnet,
               gridExtra)

# Global ggplot theme
theme_set(theme_bw() + theme(legend.position = "top"))
```

Regression is a modeling technique for predicting quantitative-valued target attributes. The goals for this tutorial are as follows:

1.  To provide examples of using different regression methods from the tidymodels package.
2.  To demonstrate the problem of model overfitting due to correlated attributes in the data.
3.  To illustrate how regularization can be used to avoid model overfitting.

## Synthetic Data Generation

To illustrate how linear regression works, we first generate a random 1-dimensional vector of predictor variables, x, from a uniform distribution. The response variable y has a linear relationship with x according to the following equation: y = -3x + 1 + epsilon, where epsilon corresponds to random noise sampled from a Gaussian distribution with mean 0 and standard deviation of 1.

```{r synthetic_data, message = FALSE}
seed <- 1
numInstances <- 200

set.seed(seed)

X <- matrix(runif(numInstances), ncol = 1)
y_true <- -3 * X + 1
y <- y_true + matrix(rnorm(numInstances), ncol = 1)

ggplot() +
  geom_point(aes(x = X, y = y), color = "black") +
  geom_line(aes(x = X, y = y_true),
            color = "blue",
            linewidth = 1) +
  ggtitle('True function: y = -3X + 1') +
  xlab('X') +
  ylab('y')
```

## Multiple Linear Regression

Given the input dataset, the following steps are performed:

1.  Split the input data into their respective training and test sets.
2.  Fit multiple linear regression to the training data.
3.  Apply the model to the test data.
4.  Evaluate the performance of the model.
5.  Post-processing: Visualizing the fitted model.

#### Step 1: Split Input Data into Training and Test Sets

```{r split_input_data, message = FALSE}
numTrain <- 20
numTest <- numInstances - numTrain

set.seed(123)

data <- tibble(X = X, y = y)

split_obj <- initial_split(data, prop = numTrain/numInstances)

train_data <- training(split_obj)
test_data <- testing(split_obj)

X_train <- train_data$X
y_train <- train_data$y

X_test <- test_data$X
y_test <- test_data$y
```

#### Step 2: Fit Regression Model to Training Set

```{r fit_regression, message = FALSE}
lin_reg_spec <- linear_reg() |> 
  set_engine("lm")

lin_reg_fit <- lin_reg_spec |> 
  fit(y ~ X, data = train_data)
```

#### Step 3: Apply Model to the Test Set

```{r apply_model, message = FALSE}
y_pred_test <- predict(lin_reg_fit, new_data = test_data) |>
  pull(.pred)
```

#### Step 4: Evaluate Model Performance on Test Set

```{r evaluate_model_perf, message = FALSE}
ggplot() +
  geom_point(aes(x = as.vector(y_test), y = y_pred_test), color = 'black') +
  ggtitle('Comparing true and predicted values for test set') +
  xlab('True values for y') +
  ylab('Predicted values for y')


eval_data <- tibble(truth = as.vector(y_test),
                    estimate = y_pred_test)

rmse_value <-
  rmse(data = eval_data,
       truth = truth,
       estimate = estimate)
r2_value <- rsq(eval_data, truth = truth, estimate = estimate)

cat("Root mean squared error =",
    sprintf("%.4f", rmse_value$.estimate),
    "\n")
cat('R-squared =', sprintf("%.4f", r2_value$.estimate), "\n")
```

#### Step 5: Post-processing

```{r post_processing, message = FALSE}
coef_values <- coef(lin_reg_fit$fit)
slope <- coef_values["X"]
intercept <- coef_values["(Intercept)"]

cat("Slope =", slope, "\n")
cat("Intercept =", intercept, "\n")

ggplot() +
  geom_point(aes(x = as.vector(X_test), y = as.vector(y_test)), color = 'black') +
  geom_line(aes(x = as.vector(X_test), y = y_pred_test),
            color = 'blue',
            linewidth = 1) +
  ggtitle(sprintf('Predicted Function: y = %.2fX + %.2f', slope, intercept)) +
  xlab('X') +
  ylab('y')
```

## Effect of Correlated Attributes

In this example, we illustrate how the presence of correlated attributes can affect the performance of regression models. Specifically, we create 4 additional variables, X2, X3, X4, and X5 that are strongly correlated with the previous variable X created in Section 5.1. The relationship between X and y remains the same as before. We then fit y against the predictor variables and compare their training and test set errors.

First, we create the correlated attributes below.

```{r correla_attr, message = FALSE}
set.seed(1)
X2 <- 0.5 * X + rnorm(numInstances, mean = 0, sd = 0.04)
X3 <- 0.5 * X2 + rnorm(numInstances, mean = 0, sd = 0.01)
X4 <- 0.5 * X3 + rnorm(numInstances, mean = 0, sd = 0.01)
X5 <- 0.5 * X4 + rnorm(numInstances, mean = 0, sd = 0.01)

plot1 <- ggplot() +
  geom_point(aes(X, X2), color = 'black') +
  xlab('X') + ylab('X2') +
  ggtitle(sprintf("Correlation between X and X2 = %.4f", cor(X[-c((numInstances -
                                                                     numTest + 1):numInstances)], X2[-c((numInstances - numTest + 1):numInstances)])))

plot2 <- ggplot() +
  geom_point(aes(X2, X3), color = 'black') +
  xlab('X2') + ylab('X3') +
  ggtitle(sprintf("Correlation between X2 and X3 = %.4f", cor(X2[-c((numInstances -
                                                                       numTest + 1):numInstances)], X3[-c((numInstances - numTest + 1):numInstances)])))

plot3 <- ggplot() +
  geom_point(aes(X3, X4), color = 'black') +
  xlab('X3') + ylab('X4') +
  ggtitle(sprintf("Correlation between X3 and X4 = %.4f", cor(X3[-c((numInstances -
                                                                       numTest + 1):numInstances)], X4[-c((numInstances - numTest + 1):numInstances)])))

plot4 <- ggplot() +
  geom_point(aes(X4, X5), color = 'black') +
  xlab('X4') + ylab('X5') +
  ggtitle(sprintf("Correlation between X4 and X5 = %.4f", cor(X4[-c((numInstances -
                                                                       numTest + 1):numInstances)], X5[-c((numInstances - numTest + 1):numInstances)])))

grid.arrange(plot1, plot2, plot3, plot4, ncol = 2)
```

Next, we create 4 additional versions of the training and test sets. The first version, X_train2 and X_test2 have 2 correlated predictor variables, X and X2. The second version, X_train3 and X_test3 have 3 correlated predictor variables, X, X2, and X3. The third version have 4 correlated variables, X, X2, X3, and X4 whereas the last version have 5 correlated variables, X, X2, X3, X4, and X5.

```{r train_test_data, message = FALSE}
train_indices <- 1:(numInstances - numTest)
test_indices <- (numInstances - numTest + 1):numInstances

X_train2 <- cbind(X[train_indices], X2[train_indices])
X_test2 <- cbind(X[test_indices], X2[test_indices])

X_train3 <-
  cbind(X[train_indices], X2[train_indices], X3[train_indices])
X_test3 <-
  cbind(X[test_indices], X2[test_indices], X3[test_indices])

X_train4 <-
  cbind(X[train_indices], X2[train_indices], X3[train_indices], X4[train_indices])
X_test4 <-
  cbind(X[test_indices], X2[test_indices], X3[test_indices], X4[test_indices])

X_train5 <-
  cbind(X[train_indices], X2[train_indices], X3[train_indices], X4[train_indices], X5[train_indices])
X_test5 <-
  cbind(X[test_indices], X2[test_indices], X3[test_indices], X4[test_indices], X5[test_indices])
```

Below, we train 4 new regression models based on the 4 versions of training and test data created in the previous step.

```{r test_reggression, message = FALSE}
train_data2 <-
  tibble(X1 = X_train2[, 1], X2 = X_train2[, 2], y = y_train)
train_data3 <-
  tibble(X1 = X_train3[, 1],
         X2 = X_train3[, 2],
         X3 = X_train3[, 3],
         y = y_train)
train_data4 <-
  tibble(
    X1 = X_train4[, 1],
    X2 = X_train4[, 2],
    X3 = X_train4[, 3],
    X4 = X_train4[, 4],
    y = y_train
  )
train_data5 <-
  tibble(
    X1 = X_train5[, 1],
    X2 = X_train5[, 2],
    X3 = X_train5[, 3],
    X4 = X_train5[, 4],
    X5 = X_train5[, 5],
    y = y_train
  )

regr2_spec <- linear_reg() %>% set_engine("lm")
regr2_fit <- regr2_spec %>% fit(y ~ X1 + X2, data = train_data2)

regr3_spec <- linear_reg() %>% set_engine("lm")
regr3_fit <-
  regr3_spec %>% fit(y ~ X1 + X2 + X3, data = train_data3)

regr4_spec <- linear_reg() %>% set_engine("lm")
regr4_fit <-
  regr4_spec %>% fit(y ~ X1 + X2 + X3 + X4, data = train_data4)

regr5_spec <- linear_reg() %>% set_engine("lm")
regr5_fit <-
  regr5_spec %>% fit(y ~ X1 + X2 + X3 + X4 + X5, data = train_data5)
```

All 4 versions of the regression models are then applied to the training and test sets.

```{r new_data_pred, message = FALSE}
new_train_data2 <- setNames(as.data.frame(X_train2), c("X1", "X2"))
new_test_data2 <- setNames(as.data.frame(X_test2), c("X1", "X2"))

new_train_data3 <-
  setNames(as.data.frame(X_train3), c("X1", "X2", "X3"))
new_test_data3 <-
  setNames(as.data.frame(X_test3), c("X1", "X2", "X3"))

new_train_data4 <-
  setNames(as.data.frame(X_train4), c("X1", "X2", "X3", "X4"))
new_test_data4 <-
  setNames(as.data.frame(X_test4), c("X1", "X2", "X3", "X4"))

new_train_data5 <-
  setNames(as.data.frame(X_train5), c("X1", "X2", "X3", "X4", "X5"))
new_test_data5 <-
  setNames(as.data.frame(X_test5), c("X1", "X2", "X3", "X4", "X5"))

y_pred_train2 <- predict(regr2_fit, new_data = new_train_data2)
y_pred_test2 <- predict(regr2_fit, new_data = new_test_data2)

y_pred_train3 <- predict(regr3_fit, new_data = new_train_data3)
y_pred_test3 <- predict(regr3_fit, new_data = new_test_data3)

y_pred_train4 <- predict(regr4_fit, new_data = new_train_data4)
y_pred_test4 <- predict(regr4_fit, new_data = new_test_data4)

y_pred_train5 <- predict(regr5_fit, new_data = new_train_data5)
y_pred_test5 <- predict(regr5_fit, new_data = new_test_data5)
```

For post-processing, we compute both the training and test errors of the models. We can also show the resulting model and the sum of the absolute weights of the regression coefficients, i.e., $\sum_{j=0}^d |w_j|$, where $d$ is the number of predictor attributes.

```{r coef_functions, message=FALSE}
get_coef <- function(model) {
  coef <- coefficients(model$fit)
  coef
}

calculate_rmse <- function(actual, predicted) {
  rmse <- sqrt(mean((actual - predicted) ^ 2))
  rmse
}
```

```{r results_regression, message=FALSE}
results <- tibble(
  Model = c(
    sprintf("%.2f X + %.2f", get_coef(regr2_fit)['X1'], get_coef(regr2_fit)['(Intercept)']),
    sprintf(
      "%.2f X + %.2f X2 + %.2f",
      get_coef(regr3_fit)['X1'],
      get_coef(regr3_fit)['X2'],
      get_coef(regr3_fit)['(Intercept)']
    ),
    sprintf(
      "%.2f X + %.2f X2 + %.2f X3 + %.2f",
      get_coef(regr4_fit)['X1'],
      get_coef(regr4_fit)['X2'],
      get_coef(regr4_fit)['X3'],
      get_coef(regr4_fit)['(Intercept)']
    ),
    sprintf(
      "%.2f X + %.2f X2 + %.2f X3 + %.2f X4 + %.2f",
      get_coef(regr5_fit)['X1'],
      get_coef(regr5_fit)['X2'],
      get_coef(regr5_fit)['X3'],
      get_coef(regr5_fit)['X4'],
      get_coef(regr5_fit)['(Intercept)']
    )
  ),
  
  Train_error = c(
    calculate_rmse(y_train, y_pred_train2$.pred),
    calculate_rmse(y_train, y_pred_train3$.pred),
    calculate_rmse(y_train, y_pred_train4$.pred),
    calculate_rmse(y_train, y_pred_train5$.pred)
  ),
  
  Test_error = c(
    calculate_rmse(y_test, y_pred_test2$.pred),
    calculate_rmse(y_test, y_pred_test3$.pred),
    calculate_rmse(y_test, y_pred_test4$.pred),
    calculate_rmse(y_test, y_pred_test5$.pred)
  ),
  
  Sum_of_Absolute_Weights = c(sum(abs(
    get_coef(regr2_fit)
  )),
  sum(abs(
    get_coef(regr3_fit)
  )),
  sum(abs(
    get_coef(regr4_fit)
  )),
  sum(abs(
    get_coef(regr5_fit)
  )))
)
```

```{r plot_results_regr, message=FALSE}
ggplot(results, aes(x = Sum_of_Absolute_Weights)) +
  geom_line(aes(y = Train_error, color = "Train error"), linetype = "solid") +
  geom_line(aes(y = Test_error, color = "Test error"), linetype = "dashed") +
  labs(x = "Sum of Absolute Weights", y = "Error rate") +
  theme_minimal()

results
```

## **Ridge Regression**

```{r ridge_regression, message=FALSE}
train_data <- tibble(y = y_train, X_train5)
test_data <- tibble(y = y_test, X_test5)

ridge_spec <- linear_reg(penalty = 0.4, mixture = 1) %>%
  set_engine("glmnet")

ridge_fit <- ridge_spec %>%
  fit(y ~ ., data = train_data)

y_pred_train_ridge <-
  predict(ridge_fit, new_data = train_data)$.pred
y_pred_test_ridge <- predict(ridge_fit, new_data = test_data)$.pred

y_pred_train_ridge <-
  predict(ridge_fit, new_data = train_data)$.pred
y_pred_test_ridge <- predict(ridge_fit, new_data = train_data)$.pred

calculate_rmse <- function(actual, predicted) {
  rmse <- sqrt(mean((actual - predicted) ^ 2))
  rmse
}

ridge_coef <- coefficients(ridge_fit$fit)

model6 <-
  sprintf(
    "%.2f X + %.2f X2 + %.2f X3 + %.2f X4 + %.2f X5 + %.2f",
    ridge_coef[2],
    ridge_coef[3],
    ridge_coef[4],
    ridge_coef[5],
    ridge_coef[6],
    ridge_coef[1]
  )

values6 <- tibble(
  Model = model6,
  Train_error = calculate_rmse(y_train, y_pred_train_ridge),
  Test_error = calculate_rmse(y_test, y_pred_test_ridge),
  Sum_of_Absolute_Weights = sum(abs(ridge_coef))
)

final_results <- bind_rows(results, values6)

final_results
```

## **Lasso Regression**

```{r lasso_reggression, message=FALSE}
lasso_spec <- linear_reg(penalty = 0.02, mixture = 1) %>%
  set_engine("glmnet")

train_data <-
  tibble(
    y = y_train,
    X1 = X_train5[, 1],
    X2 = X_train5[, 2],
    X3 = X_train5[, 3],
    X4 = X_train5[, 4],
    X5 = X_train5[, 5]
  )

lasso_fit <- lasso_spec %>%
  fit(y ~ ., data = train_data)

lasso_coefs <- lasso_fit$fit$beta[, 1]

y_pred_train_lasso <-
  predict(lasso_fit, new_data = train_data)$.pred
y_pred_test_lasso <-
  predict(
    lasso_fit,
    new_data = tibble(
      X1 = X_test5[, 1],
      X2 = X_test5[, 2],
      X3 = X_test5[, 3],
      X4 = X_test5[, 4],
      X5 = X_test5[, 5]
    )
  )$.pred

model7 <-
  sprintf(
    "%.2f X + %.2f X2 + %.2f X3 + %.2f X4 + %.2f X5 + %.2f",
    lasso_coefs[2],
    lasso_coefs[3],
    lasso_coefs[4],
    lasso_coefs[5],
    lasso_coefs[6],
    lasso_fit$fit$a0[1]
  )

values7 <- c(model7,
             sqrt(mean((
               y_train - y_pred_train_lasso
             ) ^ 2)),
             sqrt(mean((
               y_test - y_pred_test_lasso
             ) ^ 2)),
             sum(abs(lasso_coefs[-1])) + abs(lasso_fit$fit$a0[1]))

lasso_results <- tibble(
  Model = "Lasso",
  `Train error` = values7[2],
  `Test error` = values7[3],
  `Sum of Absolute Weights` = values7[4]
)

lasso_results
```
